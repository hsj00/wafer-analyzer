# modules/ml_anomaly.py
# ML ì´ìƒ íƒì§€ ëª¨ë“ˆ (PCA + IsolationForest)
# ë‹¤ì¤‘ ì›¨ì´í¼ ë§µì„ ë²¡í„°í™”í•´ ì´ìƒ ì›¨ì´í¼ë¥¼ ìë™ íƒì§€
#
# pip install scikit-learn
#
# =============================================================================
# [ì„¤ê³„ ê²°ì • ê·¼ê±°]
# =============================================================================
#
# â‘  ì†Œê·œëª¨ ìƒ˜í”Œ(n < 5) IsolationForest ì‹ ë¢°ë„ ëŒ€ì‘
#    contamination=0.10 + n=4 â†’ 0.4ê°œ = floor(0) â†’ ì´ìƒ íƒì§€ 0ê°œ â†’ ì˜ë¯¸ ì—†ìŒ.
#    ëŒ€ì‘:
#      n < 3  â†’ st.warning + return (UI ì§„ì… ì°¨ë‹¨, ìµœì†Œ ë¹„êµ ë¶ˆê°€)
#      3 â‰¤ n < 5 â†’ "ê²°ê³¼ ì‹ ë¢°ë„ ë‚®ìŒ" ê²½ê³  + ê³„ì† í—ˆìš©
#      contamination ìë™ ìƒí•œ ì¡°ì •: min(contamination, (n-1)/n - 0.01)
#        â†’ n=4, contamination=0.30 â†’ min(0.30, 0.74) = 0.30 ê·¸ëŒ€ë¡œ
#        â†’ n=3, contamination=0.40 â†’ min(0.40, 0.65) = 0.40 ê·¸ëŒ€ë¡œ
#        â†’ n=5, contamination=0.80 â†’ min(0.80, 0.79) = 0.79 (ìë™ ì¡°ì •)
#
# â‘¡ ZI NaN ì²˜ë¦¬ ì „ëµ: Z-score ì •ê·œí™” í›„ 0 ëŒ€ì²´
#    ZI NaN ë°œìƒ: circular mask (ì›¨ì´í¼ ì› ë°– ì˜ì—­)
#    ëª¨ë“  ì›¨ì´í¼ê°€ ë™ì¼í•œ circular íŒ¨í„´ â†’ ì™¸ë¶€ í”½ì…€ì„ 0(ì •ê·œí™” ê¸°ì¤€ê°’)ìœ¼ë¡œ í†µì¼
#    ì²˜ë¦¬ ìˆœì„œ:
#      1. valid_mask = ~np.isnan(ZI)  (ì›¨ì´í¼ ë‚´ë¶€ í”½ì…€ë§Œ)
#      2. zi_mean, zi_std = ZI[valid_mask].mean(), ZI[valid_mask].std()
#      3. ZI_norm = (ZI - zi_mean) / (zi_std + 1e-10)  (ì „ì²´ ë°°ì—´)
#      4. ZI_norm[~valid_mask] = 0.0  (ì •ê·œí™” í›„ ì™¸ë¶€ í”½ì…€ = 0 = í‰ê·  ìˆ˜ì¤€)
#    â†’ ì •ê·œí™” ì „ì— 0 ëŒ€ì²´í•˜ë©´ zi_mean/stdê°€ ì™¸ë¶€ 0ì— ì˜í•´ ì˜¤ì—¼ë¨ (NG)
#    â†’ ì •ê·œí™” í›„ 0 ëŒ€ì²´: ì™¸ë¶€ í”½ì…€ì´ ì •ê·œí™” ê¸°ì¤€(=0)ì— ìœ„ì¹˜ (OK)
#
# â‘¢ contamination ë³€ê²½ ì‹œ PCA ì¬ì‚¬ìš© ìºì‹œ ì „ëµ
#    PCA ë¹„ìš©: ë†’ìŒ (SVD ë¶„í•´, ì›¨ì´í¼ ìˆ˜Ã—í•´ìƒë„Â² í–‰ë ¬)
#    IF ë¹„ìš©:  ë‚®ìŒ (íŠ¸ë¦¬ ì•™ìƒë¸”, ë¹ ë¥¸ ì¬ì‹¤í–‰ ê°€ëŠ¥)
#    ìºì‹œ í‚¤ ì„¤ê³„:
#      pca_key = (tuple(sorted(names)), resolution)
#        â†’ ì›¨ì´í¼ ëª©ë¡ or í•´ìƒë„ ë³€ê²½ ì‹œ PCA ë¬´íš¨í™”
#        â†’ contamination ë³€ê²½ë§Œìœ¼ë¡œëŠ” pca_key ë¶ˆë³€ â†’ PCA ì¬ì‚¬ìš©
#      if_key  = (pca_key, contamination)
#        â†’ contamination ë³€ê²½ â†’ if_key ë³€ê²½ â†’ IFë§Œ ì¬ì‹¤í–‰
#    ë‘ í‚¤ë¥¼ session_state["ml_pca_key"], session_state["ml_if_key"]ì— ì €ì¥
#    â†’ ë²„íŠ¼ í´ë¦­ ì‹œ í‚¤ ë¹„êµë¡œ ì¬ì‹¤í–‰ ì—¬ë¶€ ê²°ì •
#
# â‘£ ndarray/@st.cache_data ë¹„í˜¸í™˜ì„±
#    np.ndarrayëŠ” mutable â†’ @st.cache_data ì¸ìë¡œ ë¶ˆê°€
#    PCA ê²°ê³¼(ndarray)ì™€ IF ê²°ê³¼(ndarray)ëŠ” session_stateì— ì§ì ‘ ì €ì¥
#    â†’ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ì—ì„œë§Œ ì¬ê³„ì‚° â†’ session_stateì— ë³´ì¡´
# =============================================================================

# â”€â”€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import glob
import hashlib
import os

# â”€â”€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st

# â”€â”€ wafer_app_global í•µì‹¬ í•¨ìˆ˜ import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from app import _default_col_index   # ì»¬ëŸ¼ ê¸°ë³¸ê°’ íƒìƒ‰ (ë°ì´í„°ì…‹ ì¶”ê°€ UI)
from app import apply_col_mapping    # x/y/data ì»¬ëŸ¼ í‘œì¤€í™” (ë°ì´í„°ì…‹ ì¶”ê°€ UI)
from app import calculate_stats      # GPC íŒ¨í„´ ë¶„ë¥˜ìš© í†µê³„
from app import create_2d_heatmap   # compact=Trueë¡œ ì´ìƒ ì›¨ì´í¼ ë¯¸ë¦¬ë³´ê¸°
from app import get_sheet_names      # Excel ì‹œíŠ¸ ëª©ë¡ (ë°ì´í„°ì…‹ ì¶”ê°€ UI)
from app import get_wafer_grid       # ë¶ˆê·œì¹™ ì‚°ì  â†’ ê· ì¼ ê·¸ë¦¬ë“œ ë³´ê°„ (@st.cache_data)
from app import load_file_cached     # CSV/Excel ë¡œë“œ (ë°ì´í„°ì…‹ ì¶”ê°€ UI)

# =============================================================================
# scikit-learn ê°€ìš©ì„± íƒì§€ (ëª¨ë“ˆ ë¡œë”© ì‹œ 1íšŒ)
# =============================================================================

try:
    from sklearn.decomposition import PCA
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    _SKLEARN_OK = True
except ImportError:
    _SKLEARN_OK = False
    # íƒ€ì… íŒíŠ¸ìš© ë”ë¯¸ í´ë˜ìŠ¤ (import ì‹¤íŒ¨ ì‹œ NameError ë°©ì§€)
    PCA              = None   # type: ignore
    IsolationForest  = None   # type: ignore
    StandardScaler   = None   # type: ignore


# =============================================================================
# session_state í‚¤ ìƒìˆ˜ (prefix: "ml_")
# =============================================================================
_SS_PCA_KEY     = "ml_pca_key"       # PCA ìºì‹œ í‚¤ (tuple â†’ hash)
_SS_PCA_RESULT  = "ml_pca_result"    # PCA ê²°ê³¼ dict
_SS_IF_KEY      = "ml_if_key"        # IF ìºì‹œ í‚¤ tuple
_SS_IF_RESULT   = "ml_if_result"     # IF ê²°ê³¼ dict
_SS_NAMES       = "ml_names"         # ìœ íš¨ ì›¨ì´í¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
_SS_PATTERNS    = "ml_patterns"      # íŒ¨í„´ ë¶„ë¥˜ ê²°ê³¼ dict
_SS_CONTAM      = "ml_contamination" # ë§ˆì§€ë§‰ ì‹¤í–‰ëœ contamination ê°’
_SS_RESOLUTION  = "ml_resolution"    # ë§ˆì§€ë§‰ ì‹¤í–‰ëœ í•´ìƒë„ ê°’
# â”€â”€ ë°ì´í„°ì…‹ ê´€ë¦¬ (ì‹ ê·œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SS_DATASETS    = "ml_datasets"      # ML íƒ­ ì „ìš© ë°ì´í„°ì…‹ [{name, df_json}, ...]
_SS_APP_HASH    = "ml_app_hash"      # ì•± ì œê³µ datasets ì´ë¦„ ëª©ë¡ì˜ hash (ë™ê¸°í™” ê°ì§€)


# =============================================================================
# ì´ìƒ íŒ¨í„´ ë¶„ë¥˜ ì„ê³„ê°’ ìƒìˆ˜
# =============================================================================
_HOTSPOT_RATIO   = 4.0   # max > mean + kÃ—std â†’ Hotspot (k=4 â†’ 4Ïƒ ì´ìƒ)
_RING_CE_RATIO   = 0.05  # |center-edge|/global_mean > 5% â†’ Ring í›„ë³´
_EDGE_DEG_RATIO  = 0.90  # edge_mean < center_mean Ã— 0.90 â†’ Edge Degradation
_GRAD_CORR_THR   = 0.40  # |corr| > 0.40 â†’ Gradient ë°©í–¥ì„± ìˆìŒ
_NORMAL_UNIF_THR = 2.0   # Uniformity(%) < 2% â†’ Normal


# =============================================================================
# [í•¨ìˆ˜ 1] prepare_wafer_features
# =============================================================================

def prepare_wafer_features(
    maps_data: list,
    resolution: int = 50,
) -> tuple:
    """
    ì—¬ëŸ¬ ì›¨ì´í¼ ë°ì´í„°ì—ì„œ ML íŠ¹ì„± í–‰ë ¬ì„ ì¶”ì¶œ.

    [ì²˜ë¦¬ íë¦„]
    for each wafer:
      1. get_wafer_grid(df_json, resolution) â†’ ZI [resolution Ã— resolution]
      2. ë³´ê°„ ì‹¤íŒ¨(ZI ì „ì²´ NaN) â†’ valid_mask=False, ê±´ë„ˆëœ€
      3. Z-score ì •ê·œí™” (ìœ íš¨ í”½ì…€ë§Œ ì‚¬ìš©):
           valid = ~isnan(ZI)
           zi_norm = (ZI - ZI[valid].mean()) / (ZI[valid].std() + 1e-10)
      4. ì™¸ë¶€ í”½ì…€(NaN ìœ„ì¹˜) â†’ 0ìœ¼ë¡œ ëŒ€ì²´ (ì •ê·œí™” í‰ê·  ìˆ˜ì¤€)
      5. flatten â†’ 1D ë²¡í„° (resolutionÂ²ì°¨ì›)

    [Z-score ì •ê·œí™”ë¥¼ ë¨¼ì € í•˜ëŠ” ì´ìœ ]
    0 ëŒ€ì²´ í›„ ì •ê·œí™” ì‹œ ì™¸ë¶€ 0ì´ mean/stdë¥¼ ì˜¤ì—¼ â†’ ë‚´ë¶€ ë¶„í¬ ì™œê³¡.
    ì •ê·œí™” í›„ 0 ëŒ€ì²´: ì™¸ë¶€ í”½ì…€ì´ ì •ê·œí™” ê¸°ì¤€(=0, í‰ê·  ìˆ˜ì¤€)ì— ìœ„ì¹˜ â†’ ì˜¬ë°”ë¦„.

    [StandardScaler ë¯¸ì‚¬ìš© ì´ìœ ]
    ê° ì›¨ì´í¼ë¥¼ ë…ë¦½ì ìœ¼ë¡œ Z-score ì •ê·œí™” â†’ ì ˆëŒ€ ë‘ê»˜ ì°¨ì´ê°€ ì•„ë‹Œ
    ê³µê°„ íŒ¨í„´(ë¶„í¬ í˜•íƒœ)ì— ì§‘ì¤‘. StandardScalerëŠ” í”¼ì²˜ ì»¬ëŸ¼ ë°©í–¥ ì •ê·œí™”
    (ì›¨ì´í¼ ê°„ ê°™ì€ í”½ì…€ì„ ì •ê·œí™”) â†’ ëª©ì ì´ ë‹¤ë¦„.

    ì¸ì:
        maps_data : [{"df_json": str, "name": str}, ...] ì›¨ì´í¼ ëª©ë¡
        resolution: ë³´ê°„ ê·¸ë¦¬ë“œ í•´ìƒë„ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„, ê¸°ë³¸ 50 â†’ 2500ì°¨ì›)

    ë°˜í™˜:
        (feature_matrix, valid_names, valid_mask)
        feature_matrix: ndarray (n_valid Ã— resolutionÂ²)
        valid_names   : ìœ íš¨ ì›¨ì´í¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        valid_mask    : ê° ì›¨ì´í¼ê°€ ìœ íš¨í•œì§€ bool ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ ìˆœì„œ ë³´ì¡´)
    """
    feature_rows: list[np.ndarray] = []
    valid_names: list[str]         = []
    valid_mask:  list[bool]        = []

    for wafer in maps_data:
        df_json   = wafer.get("df_json", "")
        wafer_name = wafer.get("name", "Unknown")

        try:
            # â”€â”€ ê·¸ë¦¬ë“œ ë³´ê°„ (í•˜ìœ„ ìºì‹œ get_wafer_grid ì¬ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            _, _, ZI, _ = get_wafer_grid(df_json, resolution)

            # â”€â”€ ë³´ê°„ ì‹¤íŒ¨ ì²´í¬: ZIê°€ ì „ë¶€ NaNì´ë©´ ì œì™¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            valid_pixels = ~np.isnan(ZI)
            n_valid = int(valid_pixels.sum())
            if n_valid < 3:
                # ìœ íš¨ í”½ì…€ 3ê°œ ë¯¸ë§Œ: ë³´ê°„ ì‹¤íŒ¨ ë˜ëŠ” ì˜ë¯¸ ì—†ëŠ” ë°ì´í„°
                valid_mask.append(False)
                continue

            # â”€â”€ Z-score ì •ê·œí™” (ìœ íš¨ í”½ì…€ë§Œ ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            zi_mean = float(ZI[valid_pixels].mean())
            zi_std  = float(ZI[valid_pixels].std())
            zi_norm = (ZI - zi_mean) / (zi_std + 1e-10)

            # â”€â”€ ì™¸ë¶€ í”½ì…€ì„ 0ìœ¼ë¡œ ëŒ€ì²´ (ì •ê·œí™” í›„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ì™¸ë¶€ í”½ì…€ì´ ì •ê·œí™” ê¸°ì¤€(0=í‰ê· )ì— ìœ„ì¹˜ â†’ íŠ¹ì„± ë²¡í„° ê¸¸ì´ í†µì¼
            zi_norm[~valid_pixels] = 0.0

            # â”€â”€ flatten â†’ 1D íŠ¹ì„± ë²¡í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            feature_rows.append(zi_norm.flatten())
            valid_names.append(wafer_name)
            valid_mask.append(True)

        except Exception:
            # ê°œë³„ ì›¨ì´í¼ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€ (ì „ì²´ ì¤‘ë‹¨ ë°©ì§€)
            valid_mask.append(False)
            continue

    if len(feature_rows) == 0:
        # ëª¨ë“  ì›¨ì´í¼ ì²˜ë¦¬ ì‹¤íŒ¨
        return np.empty((0, resolution * resolution)), [], valid_mask

    feature_matrix = np.vstack(feature_rows)   # (n_valid, resolutionÂ²)
    return feature_matrix, valid_names, valid_mask


# =============================================================================
# [í•¨ìˆ˜ 2] run_pca
# =============================================================================

def run_pca(feature_matrix: np.ndarray) -> dict:
    """
    íŠ¹ì„± í–‰ë ¬ì— PCAë¥¼ ì ìš©í•´ ì €ì°¨ì› í‘œí˜„ ìƒì„±.

    [@st.cache_data ë¯¸ì ìš© ì´ìœ ]
    np.ndarrayëŠ” mutable â†’ hash() ë¶ˆê°€ â†’ @st.cache_data ë¶ˆê°€.
    â†’ í˜¸ì¶œë¶€(render_anomaly_tab)ì—ì„œ ìºì‹œ í‚¤ ë¹„êµë¡œ ì¬ì‹¤í–‰ ì—¬ë¶€ ê²°ì •.
    â†’ ê²°ê³¼ëŠ” session_state["ml_pca_result"]ì— ì €ì¥.

    [n_components ê²°ì •]
    n_components = min(n_wafers - 1, 10)
    ì´ìœ :
      - PCAì˜ ìœ íš¨ ìµœëŒ€ ì„±ë¶„ ìˆ˜ = min(n_samples-1, n_features)
      - 10ê°œ ì´ìƒì˜ PCëŠ” ì‹œê°í™”(2D ì‚°ì ë„)ì— ì‚¬ìš© ì•ˆ ë˜ê³  ë©”ëª¨ë¦¬ë§Œ ë‚­ë¹„
      - n_wafers - 1: ìµœì†Œ 1ê°œì˜ ì„±ë¶„ ë³´ì¥ (n_wafers â‰¥ 2ì´ë©´)

    [explained_variance_ratio í™œìš©]
    PC1, PC2 ì¶• ë¼ë²¨ì— % í‘œì‹œ â†’ í•´ë‹¹ ì„±ë¶„ì´ ì „ì²´ ë¶„ì‚°ì˜ ëª‡ %ë¥¼ ì„¤ëª…í•˜ëŠ”ì§€
    ë°˜ë„ì²´ ê³µì •ì—ì„œ ì£¼ìš” ë³€ë™ ëª¨ë“œ(Ring, Gradient ë“±)ì˜ ê¸°ì—¬ë„ ì§ê´€ì  íŒŒì•…

    ì¸ì:
        feature_matrix: ndarray (n_wafers Ã— resolutionÂ²)

    ë°˜í™˜:
        {
          "components"            : ndarray (n_wafers Ã— n_components),
          "explained_variance_ratio": ndarray (n_components,),
          "n_components"          : int,
        }
    """
    if not _SKLEARN_OK:
        raise ImportError("scikit-learn ë¯¸ì„¤ì¹˜: 'pip install scikit-learn'")

    n_wafers   = feature_matrix.shape[0]
    n_comp     = min(n_wafers - 1, 10)  # ìœ íš¨ ìµœëŒ€ ì„±ë¶„ ìˆ˜ ìë™ ê²°ì •
    n_comp     = max(n_comp, 2)         # ìµœì†Œ 2ê°œ (2D ì‚°ì ë„ ë Œë”ë§ í•„ìš”)
    n_comp     = min(n_comp, feature_matrix.shape[1])  # íŠ¹ì„± ìˆ˜ ì´ˆê³¼ ë°©ì§€

    pca = PCA(n_components=n_comp, random_state=42)
    components = pca.fit_transform(feature_matrix)   # (n_wafers, n_comp)

    return {
        "components":               components,
        "explained_variance_ratio": pca.explained_variance_ratio_,
        "n_components":             n_comp,
    }


# =============================================================================
# [í•¨ìˆ˜ 3] run_isolation_forest
# =============================================================================

def run_isolation_forest(
    pca_components: np.ndarray,
    contamination: float,
) -> dict:
    """
    PCA ì„±ë¶„ ê³µê°„ì—ì„œ IsolationForestë¡œ ì´ìƒ ì›¨ì´í¼ íƒì§€.

    [@st.cache_data ë¯¸ì ìš© ì´ìœ ]
    np.ndarray(pca_components)ëŠ” mutable â†’ hash() ë¶ˆê°€.
    â†’ session_state["ml_if_result"]ì— ê²°ê³¼ ì €ì¥.
    â†’ contamination ë³€ê²½ ì‹œì—ë§Œ ì¬ì‹¤í–‰ (PCA ì„±ë¶„ì€ ë¶ˆë³€).

    [contamination ìƒí•œ ìë™ ì¡°ì •]
    n_wafersê°€ ì ìœ¼ë©´ contamination Ã— n_wafers < 1ì´ ë˜ì–´ ì´ìƒ íƒì§€ 0ê°œ ê°€ëŠ¥.
    â†’ max_contamination = (n_wafers - 1) / n_wafers - 0.01
    â†’ contamination = min(contamination, max_contamination)

    [anomaly score í•´ì„]
    IsolationForest.score_samples(): ë‚®ì„ìˆ˜ë¡ ì´ìƒ
    raw_score ë²”ìœ„: ìŒìˆ˜ (ì •ìƒ) ~ ë” í° ìŒìˆ˜ (ì´ìƒ)
    [0, 1] ì •ê·œí™”:
      inv = -raw_score  (ë¶€í˜¸ ë°˜ì „ â†’ ì´ìƒì¼ìˆ˜ë¡ í° ì–‘ìˆ˜)
      normalized = (inv - inv.min()) / (inv.max() - inv.min() + 1e-12)
      â†’ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì´ìƒ, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ìƒ

    ì¸ì:
        pca_components: PCA ì„±ë¶„ í–‰ë ¬ (n_wafers Ã— n_components)
        contamination : ì´ìƒ ì›¨ì´í¼ ë¹„ìœ¨ (0.05~0.30)

    ë°˜í™˜:
        {
          "predictions"    : ndarray (-1=ì´ìƒ, 1=ì •ìƒ),
          "scores"         : ndarray ([0,1] ì •ê·œí™”, ë†’ì„ìˆ˜ë¡ ì´ìƒ),
          "anomaly_indices": list[int] (ì´ìƒ ì›¨ì´í¼ ì¸ë±ìŠ¤),
          "threshold"      : float (ì„ê³„ ì ìˆ˜ = ì´ìƒ/ì •ìƒ ê²½ê³„),
          "contamination_used": float (ì‹¤ì œ ì ìš©ëœ contamination),
        }
    """
    if not _SKLEARN_OK:
        raise ImportError("scikit-learn ë¯¸ì„¤ì¹˜: 'pip install scikit-learn'")

    n_wafers = pca_components.shape[0]

    # contamination ìƒí•œ ìë™ ì¡°ì • (IsolationForest ë‚´ë¶€ ì œì•½: < 0.5)
    max_contamination = min(0.49, (n_wafers - 1) / n_wafers - 0.01)
    contamination_used = min(contamination, max_contamination)

    clf = IsolationForest(
        contamination=contamination_used,
        n_estimators=200,    # 200íŠ¸ë¦¬: ì•ˆì •ì„±ê³¼ ì†ë„ì˜ ê· í˜•
        random_state=42,     # ì¬í˜„ì„± ë³´ì¥
        n_jobs=-1,           # ëª¨ë“  CPU ì½”ì–´ í™œìš© (ë³‘ë ¬ íŠ¸ë¦¬ êµ¬ì¶•)
    )
    clf.fit(pca_components)

    predictions  = clf.predict(pca_components)   # 1=ì •ìƒ, -1=ì´ìƒ
    raw_scores   = clf.score_samples(pca_components)  # ë‚®ì„ìˆ˜ë¡ ì´ìƒ

    # [0, 1] ì •ê·œí™”: ë¶€í˜¸ ë°˜ì „ í›„ min-max
    # ì •ê·œí™” í›„: 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì´ìƒ, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ìƒ
    inv        = -raw_scores
    s_min, s_max = inv.min(), inv.max()
    norm_scores  = (inv - s_min) / (s_max - s_min + 1e-12)

    # ì´ìƒ/ì •ìƒ ê²½ê³„ ì ìˆ˜ ê³„ì‚°
    # IsolationForestì—ì„œ predictions=-1ì¸ ìƒ˜í”Œë“¤ì˜ ì •ê·œí™” ì ìˆ˜ ìµœì†Ÿê°’
    anom_mask = (predictions == -1)
    threshold = float(norm_scores[anom_mask].min()) if anom_mask.any() else 1.0

    anomaly_indices = [int(i) for i in np.where(anom_mask)[0]]

    return {
        "predictions":        predictions,
        "scores":             norm_scores,
        "anomaly_indices":    anomaly_indices,
        "threshold":          threshold,
        "contamination_used": contamination_used,
    }


# =============================================================================
# [í•¨ìˆ˜ 4] classify_anomaly_pattern
# =============================================================================

def classify_anomaly_pattern(df_json: str) -> str:
    """
    ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì›¨ì´í¼ ë§µ ì´ìƒ íŒ¨í„´ì„ ìë™ ë¶„ë¥˜.

    [ë¶„ë¥˜ ìš°ì„ ìˆœìœ„]
    1. Normal    : Uniformity(%) < 2% â†’ ê³µì • ì´ìƒ ì—†ìŒ
    2. Hotspot   : max > mean + 4Ïƒ   â†’ ì¢ì€ ì˜ì—­ ê¸‰ê²©í•œ í”¼í¬ (íŒŒí‹°í´, ìŠ¤í¬ë˜ì¹˜)
    3. Ring      : |center - edge| / mean > 5% AND ë‹¨ì¡°ì„± ìœ„ë°˜
    4. Edge Deg  : edge_mean < center_mean Ã— 0.90
    5. X-Gradient: |corr(x, data)| > 0.40 AND > |corr(y, data)|
    6. Y-Gradient: |corr(y, data)| > 0.40 AND > |corr(x, data)|
    7. Global Shift: ìœ„ ì–´ëŠ ê²ƒë„ ì•„ë‹˜ + Uniformity(%) > 5%
    8. Mixed     : ìœ„ ì–´ëŠ ê²ƒë„ í•´ë‹¹ ì—†ìŒ

    [3êµ¬ì—­ ì •ì˜]
    Center: r < radius Ã— 0.30
    Edge:   r â‰¥ radius Ã— 0.70

    ì¸ì:
        df_json: "x","y","data" ì»¬ëŸ¼ JSON (í‘œì¤€ êµ¬ì¡°)

    ë°˜í™˜:
        ë¶„ë¥˜ ë¬¸ìì—´: "Normal" / "Hotspot" / "Ring" / "Edge Degradation" /
                    "X-Gradient" / "Y-Gradient" / "Global Shift" / "Mixed"
    """
    try:
        df = pd.read_json(df_json).dropna(subset=["x", "y", "data"])
        if len(df) < 5:
            return "ë°ì´í„° ë¶€ì¡±"

        x    = df["x"].values
        y    = df["y"].values
        data = df["data"].values.astype(float)

        r      = np.sqrt(x ** 2 + y ** 2)
        radius = r.max()

        mean_val = float(np.nanmean(data))
        std_val  = float(np.nanstd(data))
        max_val  = float(np.nanmax(data))

        # Uniformity(%) = Ïƒ/Î¼ Ã— 100
        uniformity = (std_val / mean_val * 100) if mean_val != 0 else float("inf")

        # â”€â”€ 1. Normal: ê· ì¼ë„ 2% ì´ë‚´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if uniformity < _NORMAL_UNIF_THR:
            return "Normal"

        # â”€â”€ 2. Hotspot: ê·¹ë‹¨ í”¼í¬ (4Ïƒ ì´ìƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if mean_val != 0 and max_val > mean_val + _HOTSPOT_RATIO * std_val:
            return "Hotspot"

        # â”€â”€ 3êµ¬ì—­ í‰ê·  ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        center_mask = r <  radius * 0.30
        mid_mask    = (r >= radius * 0.30) & (r < radius * 0.70)
        edge_mask   = r >= radius * 0.70

        center_mean = float(np.nanmean(data[center_mask])) if center_mask.any() else np.nan
        mid_mean    = float(np.nanmean(data[mid_mask]))    if mid_mask.any()    else np.nan
        edge_mean   = float(np.nanmean(data[edge_mask]))   if edge_mask.any()   else np.nan

        # â”€â”€ 3. Ring: ì¤‘ì‹¬-ê°€ì¥ìë¦¬ ì°¨ì´ + ë‹¨ì¡°ì„± ìœ„ë°˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if (not np.isnan(center_mean) and not np.isnan(edge_mean)
                and mean_val != 0):
            ce_diff = abs(center_mean - edge_mean) / mean_val
            if ce_diff > _RING_CE_RATIO:
                # ë‹¨ì¡°ì„± ì²´í¬: center â†’ mid â†’ edge ë°©í–¥ì´ ì¼ê´€ë˜ì§€ ì•Šìœ¼ë©´ Ring
                if (not np.isnan(mid_mean)):
                    center_to_mid = mid_mean - center_mean
                    mid_to_edge   = edge_mean - mid_mean
                    # ë°©í–¥ ë°˜ì „: Centerâ†‘Midâ†“Edge ë˜ëŠ” Centerâ†“Midâ†‘Edge â†’ Ring
                    if (center_to_mid * mid_to_edge) < 0:
                        return "Ring"

        # â”€â”€ 4. Edge Degradation: ê°€ì¥ìë¦¬ê°€ ì¤‘ì‹¬ë³´ë‹¤ ë‚®ìŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if (not np.isnan(center_mean) and not np.isnan(edge_mean)
                and center_mean != 0):
            if edge_mean < center_mean * _EDGE_DEG_RATIO:
                return "Edge Degradation"

        # â”€â”€ 5 & 6. Gradient: X ë˜ëŠ” Y ë°©í–¥ ì„ í˜• ìƒê´€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if std_val > 0:
            corr_x = float(np.corrcoef(x, data)[0, 1]) if len(x) > 2 else 0.0
            corr_y = float(np.corrcoef(y, data)[0, 1]) if len(y) > 2 else 0.0

            abs_cx = abs(corr_x)
            abs_cy = abs(corr_y)

            if abs_cx > _GRAD_CORR_THR or abs_cy > _GRAD_CORR_THR:
                if abs_cx >= abs_cy:
                    return "X-Gradient"
                else:
                    return "Y-Gradient"

        # â”€â”€ 7. Global Shift: íŒ¨í„´ ì—†ì´ ì „ì²´ ê· ì¼ë„ ì´íƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if uniformity > 5.0:
            return "Global Shift"

        # â”€â”€ 8. Mixed: ë¶„ë¥˜ ë¶ˆê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        return "Mixed"

    except Exception:
        return "ë¶„ë¥˜ ì‹¤íŒ¨"


# =============================================================================
# [í•¨ìˆ˜ 5] create_pca_scatter
# =============================================================================

def create_pca_scatter(
    pca_result: dict,
    if_result: dict,
    wafer_names: list,
    df_jsons: list,   # íŒ¨í„´ ë¶„ë¥˜ hoverì— ì‚¬ìš©
) -> go.Figure:
    """
    PCA ê²°ê³¼ PC1-PC2 2D ì‚°ì ë„ ìƒì„±.

    [ë§ˆì»¤ ì„¤ê³„]
    ì •ìƒ ì›¨ì´í¼: íŒŒë€ ì›(circle), ë°˜íˆ¬ëª…
    ì´ìƒ ì›¨ì´í¼: ë¹¨ê°„ X(x), ë¶ˆíˆ¬ëª…
    ë§ˆì»¤ í¬ê¸°: anomaly scoreì— ë°˜ë¹„ë¡€
      â†’ score ë†’ì„ìˆ˜ë¡(ì´ìƒì¼ìˆ˜ë¡) ë” í¬ê²Œ â†’ ì‹œê°ì  ê°•ì¡°

    [hover ì •ë³´]
    ì›¨ì´í¼ ì´ë¦„ + anomaly score + ë¶„ë¥˜ íŒ¨í„´ + PC1/PC2 ê°’

    [ì¶• ë¼ë²¨]
    "PC1 (42.3%)": ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ í¬í•¨ â†’ ì£¼ìš” ë³€ë™ ëª¨ë“œ ê¸°ì—¬ë„ í‘œì‹œ

    ì¸ì:
        pca_result  : run_pca() ë°˜í™˜ dict
        if_result   : run_isolation_forest() ë°˜í™˜ dict
        wafer_names : ìœ íš¨ ì›¨ì´í¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        df_jsons    : ê° ì›¨ì´í¼ì˜ df_json (íŒ¨í„´ ë¶„ë¥˜ìš©)

    ë°˜í™˜:
        go.Figure: PCA ì‚°ì ë„
    """
    components = pca_result["components"]        # (n_wafers, n_comp)
    ev_ratio   = pca_result["explained_variance_ratio"]  # (n_comp,)
    predictions = if_result["predictions"]       # (n_wafers,)
    scores      = if_result["scores"]            # (n_wafers,) [0,1]
    threshold   = if_result["threshold"]

    pc1 = components[:, 0]
    pc2 = components[:, 1] if components.shape[1] > 1 else np.zeros_like(pc1)

    pc1_pct = ev_ratio[0] * 100 if len(ev_ratio) > 0 else 0
    pc2_pct = ev_ratio[1] * 100 if len(ev_ratio) > 1 else 0

    fig = go.Figure()

    # â”€â”€ ì •ìƒ/ì´ìƒ ë¶„ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    is_anomaly = (predictions == -1)
    norm_mask  = ~is_anomaly
    anom_mask  = is_anomaly

    # â”€â”€ ë§ˆì»¤ í¬ê¸° ê³„ì‚° (scoreì— ë°˜ë¹„ë¡€ â†’ ì´ìƒì¼ìˆ˜ë¡ í¬ê²Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì •ìƒ: 8~14px, ì´ìƒ: 14~22px
    def _score_to_size(s_arr: np.ndarray, base: float, scale: float) -> list:
        return [float(base + score * scale) for score in s_arr]

    # â”€â”€ ì •ìƒ ì›¨ì´í¼ trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if norm_mask.any():
        norm_idx   = np.where(norm_mask)[0]
        norm_names = [wafer_names[i] for i in norm_idx]
        norm_sizes = _score_to_size(scores[norm_mask], base=8, scale=6)

        # íŒ¨í„´ ë¶„ë¥˜
        norm_patterns = []
        for i in norm_idx:
            if i < len(df_jsons):
                norm_patterns.append(classify_anomaly_pattern(df_jsons[i]))
            else:
                norm_patterns.append("N/A")

        fig.add_trace(go.Scatter(
            x=pc1[norm_mask],
            y=pc2[norm_mask],
            mode="markers",
            name="ì •ìƒ ì›¨ì´í¼",
            text=norm_names,
            customdata=np.column_stack([
                scores[norm_mask].round(4),
                norm_patterns,
            ]),
            marker=dict(
                symbol="circle",
                size=norm_sizes,
                color="rgba(31, 119, 180, 0.70)",    # íŒŒë€ ë°˜íˆ¬ëª…
                line=dict(width=1, color="rgba(31, 119, 180, 0.90)"),
            ),
            hovertemplate=(
                "<b>%{text}</b><br>"
                "PC1: %{x:.4f}<br>"
                "PC2: %{y:.4f}<br>"
                "ì´ìƒ ì ìˆ˜: %{customdata[0]}<br>"
                "íŒ¨í„´: %{customdata[1]}"
                "<extra>ì •ìƒ</extra>"
            ),
        ))

    # â”€â”€ ì´ìƒ ì›¨ì´í¼ trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if anom_mask.any():
        anom_idx   = np.where(anom_mask)[0]
        anom_names = [wafer_names[i] for i in anom_idx]
        anom_sizes = _score_to_size(scores[anom_mask], base=14, scale=10)

        # íŒ¨í„´ ë¶„ë¥˜
        anom_patterns = []
        for i in anom_idx:
            if i < len(df_jsons):
                anom_patterns.append(classify_anomaly_pattern(df_jsons[i]))
            else:
                anom_patterns.append("N/A")

        fig.add_trace(go.Scatter(
            x=pc1[anom_mask],
            y=pc2[anom_mask],
            mode="markers",
            name="ì´ìƒ ì›¨ì´í¼",
            text=anom_names,
            customdata=np.column_stack([
                scores[anom_mask].round(4),
                anom_patterns,
            ]),
            marker=dict(
                symbol="x",
                size=anom_sizes,
                color="rgba(214, 39, 40, 0.90)",     # ë¹¨ê°„ ë¶ˆíˆ¬ëª…
                line=dict(width=2.5, color="rgba(214, 39, 40, 1.0)"),
            ),
            hovertemplate=(
                "<b>%{text}</b><br>"
                "PC1: %{x:.4f}<br>"
                "PC2: %{y:.4f}<br>"
                "ì´ìƒ ì ìˆ˜: %{customdata[0]}<br>"
                "íŒ¨í„´: %{customdata[1]}"
                "<extra>âš ï¸ ì´ìƒ</extra>"
            ),
        ))

    # â”€â”€ ë ˆì´ì•„ì›ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fig.update_layout(
        title=dict(text="PCA ì´ìƒ íƒì§€ ì‚°ì ë„", x=0.5, font=dict(size=14)),
        xaxis=dict(
            title=f"PC1 ({pc1_pct:.1f}%)",
            showgrid=True,
            gridcolor="rgba(200,200,200,0.5)",
            zeroline=True,
            zerolinecolor="rgba(150,150,150,0.5)",
            zerolinewidth=1,
        ),
        yaxis=dict(
            title=f"PC2 ({pc2_pct:.1f}%)",
            showgrid=True,
            gridcolor="rgba(200,200,200,0.5)",
            zeroline=True,
            zerolinecolor="rgba(150,150,150,0.5)",
            zerolinewidth=1,
        ),
        plot_bgcolor="white",
        paper_bgcolor="white",
        height=420,
        margin=dict(l=60, r=20, t=50, b=50),
        legend=dict(
            x=0.01,
            y=0.99,
            bgcolor="rgba(255,255,255,0.85)",
            bordercolor="rgba(180,180,180,0.5)",
            borderwidth=1,
        ),
    )

    return fig


# =============================================================================
# [í•¨ìˆ˜ 6] create_anomaly_score_bar
# =============================================================================

def create_anomaly_score_bar(
    wafer_names: list,
    scores: np.ndarray,
    is_anomaly: np.ndarray,
) -> go.Figure:
    """
    ì›¨ì´í¼ë³„ anomaly score ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„.

    [ì‹œê° ì„¤ê³„]
    ì´ìƒ ì›¨ì´í¼: ë¹¨ê°„ìƒ‰ ë§‰ëŒ€ (ê°•ì¡°)
    ì •ìƒ ì›¨ì´í¼: íŒŒë€ìƒ‰ ë§‰ëŒ€ (ë°°ê²½)
    ì„ê³„ê°’ ìˆ˜ì§ì„ : ì´ìƒ/ì •ìƒ ê²½ê³„ ëª…ì‹œ â†’ íŒë‹¨ ê¸°ì¤€ íˆ¬ëª…í™”
    ì •ë ¬: score ë‚´ë¦¼ì°¨ìˆœ â†’ ì´ìƒ ì›¨ì´í¼ê°€ ìƒë‹¨ì— ì§‘ì¤‘

    [anomaly score ë°©í–¥]
    score = 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì´ìƒ â†’ Xì¶•ì´ ì»¤ì§ˆìˆ˜ë¡ ìœ„í—˜
    â†’ "ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ìœ„í—˜" â†’ ì§ê´€ì 

    ì¸ì:
        wafer_names: ì›¨ì´í¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        scores     : [0,1] ì •ê·œí™” anomaly scores (1=ì´ìƒ)
        is_anomaly : bool ë°°ì—´ (True=ì´ìƒ)

    ë°˜í™˜:
        go.Figure: ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„
    """
    n = len(wafer_names)
    if n == 0:
        return go.Figure()

    # score ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ì´ìƒ ì›¨ì´í¼ ìƒë‹¨ ì§‘ì¤‘)
    sort_idx   = np.argsort(scores)[::-1]
    names_sort = [wafer_names[i] for i in sort_idx]
    scores_sort = scores[sort_idx]
    is_anom_sort = is_anomaly[sort_idx]

    # ì´ìƒ/ì •ìƒ ìƒ‰ìƒ ë°°ì—´
    colors = [
        "rgba(214, 39, 40, 0.80)" if anom else "rgba(31, 119, 180, 0.60)"
        for anom in is_anom_sort
    ]

    # ì´ìƒ/ì •ìƒ ë ˆì´ë¸”
    labels = ["âš ï¸ ì´ìƒ" if a else "âœ… ì •ìƒ" for a in is_anom_sort]

    fig = go.Figure()

    # â”€â”€ ë‹¨ì¼ traceë¡œ ì „ì²´ ë§‰ëŒ€ ê·¸ë˜í”„ (ìƒ‰ìƒì€ marker.color ë°°ì—´ë¡œ ì§€ì •) â”€â”€â”€â”€â”€â”€
    fig.add_trace(go.Bar(
        y=names_sort,              # ì›¨ì´í¼ ì´ë¦„ (Yì¶•, ìˆ˜í‰ ë§‰ëŒ€ì´ë¯€ë¡œ)
        x=scores_sort,             # anomaly score (Xì¶•)
        orientation="h",           # ìˆ˜í‰ ë§‰ëŒ€
        marker=dict(
            color=colors,
            line=dict(width=0.5, color="rgba(100,100,100,0.3)"),
        ),
        text=[f"{s:.4f}" for s in scores_sort],   # ë§‰ëŒ€ ë score í…ìŠ¤íŠ¸
        textposition="outside",
        customdata=labels,
        hovertemplate=(
            "<b>%{y}</b><br>"
            "ì´ìƒ ì ìˆ˜: %{x:.4f}<br>"
            "íŒì •: %{customdata}"
            "<extra></extra>"
        ),
        name="ì´ìƒ ì ìˆ˜",
    ))

    # â”€â”€ ì„ê³„ê°’ ìˆ˜ì§ì„  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì´ìƒ/ì •ìƒ ê²½ê³„: is_anomaly ê²½ê³„ì ì—ì„œì˜ score ê°’
    if is_anomaly.any() and (~is_anomaly).any():
        anom_scores  = scores[is_anomaly]
        normal_scores = scores[~is_anomaly]
        threshold = (anom_scores.min() + normal_scores.max()) / 2

        fig.add_vline(
            x=threshold,
            line_dash="dash",
            line_color="rgba(128, 0, 0, 0.7)",
            line_width=1.5,
            annotation_text=f"ì„ê³„ê°’ {threshold:.4f}",
            annotation_position="top right",
            annotation_font=dict(size=9, color="darkred"),
        )

    # â”€â”€ ë ˆì´ì•„ì›ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì›¨ì´í¼ ìˆ˜ì— ë”°ë¼ ë†’ì´ ìë™ ì¡°ì • (ì›¨ì´í¼ë‹¹ ì•½ 30px)
    chart_height = max(300, n * 30 + 100)

    fig.update_layout(
        title=dict(text="ì›¨ì´í¼ë³„ ì´ìƒ ì ìˆ˜", x=0.5, font=dict(size=14)),
        xaxis=dict(
            title="ì´ìƒ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì´ìƒ)",
            range=[0, 1.15],    # í…ìŠ¤íŠ¸ ë ˆì´ë¸” ê³µê°„ í™•ë³´
            showgrid=True,
            gridcolor="rgba(200,200,200,0.5)",
            zeroline=True,
        ),
        yaxis=dict(
            showgrid=False,
            autorange="reversed",  # ìƒë‹¨ì— ë†’ì€ score ë°°ì¹˜ (sortì™€ ì¼ê´€ì„±)
        ),
        plot_bgcolor="white",
        paper_bgcolor="white",
        height=chart_height,
        margin=dict(l=150, r=80, t=50, b=50),   # l=150: ê¸´ ì›¨ì´í¼ ì´ë¦„ ê³µê°„
        showlegend=False,
    )

    return fig


# =============================================================================

# =============================================================================
# [í•¨ìˆ˜ 7a] _datasets_app_hash (ë‚´ë¶€ í—¬í¼)
# =============================================================================

def _datasets_app_hash(datasets: list) -> str:
    """
    ì•±ì´ ì œê³µí•˜ëŠ” datasets ëª©ë¡ì˜ ì´ë¦„ì„ ì´ìš©í•´ ì§§ì€ í•´ì‹œë¥¼ ë°˜í™˜.

    [ìš©ë„]
    ì•± ì œê³µ datasetsê°€ ë°”ë€Œì—ˆëŠ”ì§€ ê°ì§€ â†’ ë™ê¸°í™” ë°°ë„ˆ í‘œì‹œ ì—¬ë¶€ ê²°ì •.
    ì´ë¦„ë§Œ ë¹„êµí•˜ëŠ” ì´ìœ : df_json ì „ì²´ ë¹„êµëŠ” ìˆ˜ì‹­ MBê°€ ë  ìˆ˜ ìˆìŒ.
    """
    names = sorted(ds.get("name", "") for ds in datasets)
    return hashlib.md5(str(names).encode()).hexdigest()[:12]


# =============================================================================
# [í•¨ìˆ˜ 7b] _invalidate_ml_results (ë‚´ë¶€ í—¬í¼)
# =============================================================================

def _invalidate_ml_results() -> None:
    """PCA / IF ë¶„ì„ ê²°ê³¼ë¥¼ ëª¨ë‘ ì´ˆê¸°í™”í•˜ëŠ” ê³µí†µ í—¬í¼."""
    for key in (_SS_PCA_KEY, _SS_PCA_RESULT, _SS_IF_KEY,
                _SS_IF_RESULT, _SS_NAMES, _SS_PATTERNS):
        st.session_state[key] = None


# =============================================================================
# [í•¨ìˆ˜ 7c] _render_dataset_adder (ë‚´ë¶€ í—¬í¼)
# =============================================================================

def _render_dataset_adder(data_folder: str) -> None:
    """
    íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë°ì´í„°ì…‹ì„ ì¶”ê°€í•˜ëŠ” UI.

    [ë™ì‘ íë¦„]
    1. data_folder ì˜ CSV/Excel íŒŒì¼ ëª©ë¡ ìŠ¤ìº”
    2. íŒŒì¼ ì„ íƒ â†’ ì‹œíŠ¸ ì„ íƒ (Excel)
    3. X / Y / Data ì»¬ëŸ¼ ì§€ì •
    4. ì´ë¦„ ì…ë ¥ (ìë™ ìƒì„± ê¸°ë³¸ê°’)
    5. [âœ… ì¶”ê°€] ë²„íŠ¼ â†’ apply_col_mapping â†’ df_json ìƒì„± â†’ _SS_DATASETS ì¶”ê°€
    """
    file_list: list[str] = []
    if os.path.exists(data_folder):
        file_list = sorted(
            os.path.basename(f)
            for f in glob.glob(os.path.join(data_folder, "*.csv"))
               + glob.glob(os.path.join(data_folder, "*.xls*"))
        )

    if not file_list:
        st.warning(
            f"âš ï¸ `{data_folder}` í´ë”ì— CSV/Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.  \n"
            "ì›¨ì´í¼ ë§µ íƒ­ì—ì„œ í´ë”ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”."
        )
        return

    # íŒŒì¼ Â· ì‹œíŠ¸ ì„ íƒ
    fa_col, sh_col = st.columns([3, 2])
    with fa_col:
        sel_file = st.selectbox("íŒŒì¼", file_list, key="ml_add_file")
    full_path = os.path.join(data_folder, sel_file)

    try:
        sheets = get_sheet_names(full_path)
    except Exception:
        sheets = []

    with sh_col:
        if sheets:
            sel_sheet = st.selectbox("ì‹œíŠ¸", sheets, key="ml_add_sheet")
        else:
            sel_sheet = None
            st.markdown(
                "<div style='padding-top:28px;font-size:12px;color:#888;'>"
                "CSV (ì‹œíŠ¸ ì—†ìŒ)</div>",
                unsafe_allow_html=True,
            )

    # ì»¬ëŸ¼ ëª©ë¡ í™•ë³´
    try:
        df_prev  = load_file_cached(full_path, sel_sheet)
        all_cols = df_prev.columns.tolist()
    except Exception as exc:
        st.error(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {exc}")
        return

    # X / Y / Data ì»¬ëŸ¼ ì„ íƒ
    cx, cy, cd = st.columns(3)
    with cx:
        x_col = st.selectbox(
            "X ì»¬ëŸ¼", all_cols,
            index=_default_col_index(all_cols, "x", 0),
            key="ml_add_x",
        )
    with cy:
        y_col = st.selectbox(
            "Y ì»¬ëŸ¼", all_cols,
            index=_default_col_index(all_cols, "y", 1),
            key="ml_add_y",
        )
    with cd:
        data_col = st.selectbox(
            "Data ì»¬ëŸ¼", all_cols,
            index=_default_col_index(all_cols, "data", 2),
            key="ml_add_data",
        )

    # ì´ë¦„ ì…ë ¥ (ìë™ ìƒì„± ê¸°ë³¸ê°’)
    sheet_tag = f"[{sel_sheet}]" if sel_sheet else ""
    auto_name = f"{os.path.splitext(sel_file)[0]}{sheet_tag}Â·{data_col}"
    ds_name   = st.text_input("ë°ì´í„°ì…‹ ì´ë¦„", value=auto_name, key="ml_add_name")

    # ì¤‘ë³µ ì´ë¦„ ê²½ê³ 
    existing_names = [d.get("name") for d in st.session_state.get(_SS_DATASETS, [])]
    btn_disabled   = ds_name in existing_names
    if btn_disabled:
        st.warning(f"âš ï¸ '{ds_name}' ì´ë¦„ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¦„ì„ ë³€ê²½í•˜ì„¸ìš”.")

    if st.button(
        "âœ… ë°ì´í„°ì…‹ ì¶”ê°€",
        type="primary",
        key="ml_add_btn",
        disabled=btn_disabled,
        use_container_width=True,
    ):
        try:
            df_mapped = apply_col_mapping(df_prev, x_col, y_col, data_col)
            new_ds = {"name": ds_name, "df_json": df_mapped.to_json()}

            if st.session_state.get(_SS_DATASETS) is None:
                st.session_state[_SS_DATASETS] = []
            st.session_state[_SS_DATASETS].append(new_ds)

            # ìƒˆ ì›¨ì´í¼ ì¶”ê°€ â†’ PCA ìºì‹œ ë¬´íš¨í™”
            _invalidate_ml_results()
            st.success(f"âœ… '{ds_name}' ì¶”ê°€ë¨")
            st.rerun()
        except Exception as exc:
            st.error(f"âŒ ì¶”ê°€ ì‹¤íŒ¨: {exc}")


# =============================================================================
# [í•¨ìˆ˜ 7d] _render_dataset_panel (ë‚´ë¶€ í—¬í¼)
# =============================================================================

def _render_dataset_panel(
    datasets_from_app: list,
    data_folder: str,
) -> None:
    """
    ML íƒ­ ì „ìš© ë°ì´í„°ì…‹ ê´€ë¦¬ íŒ¨ë„.

    [ë ˆì´ì•„ì›ƒ]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ“‹ ë¶„ì„ ë°ì´í„°ì…‹ (Nê°œ)   [ğŸ”„ ì•± ë™ê¸°í™”]   [ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”]        â”‚
    â”œâ”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚# â”‚ ì´ë¦„ (ì´ìƒíƒì§€ ê²°ê³¼ ì•„ì´ì½˜)    â”‚ í¬ì¸íŠ¸ ìˆ˜ â”‚ ì‚­ì œ               â”‚
    â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â–¼ [â• ë°ì´í„°ì…‹ ì¶”ê°€] (expander, 3ê°œ ë¯¸ë§Œì´ë©´ ê¸°ë³¸ í¼ì¹¨)

    [ì•± ë™ê¸°í™” ì •ì±…]
    - ìµœì´ˆ ì§„ì…(_SS_DATASETS is None): ì•± ì œê³µ datasetsë¡œ ìë™ ì´ˆê¸°í™”
    - ì´í›„: ì‚¬ìš©ì ê´€ë¦¬ ë¦¬ìŠ¤íŠ¸ ìœ ì§€
    - ì•± datasets ì´ë¦„ ëª©ë¡ hashê°€ ë°”ë€Œë©´ [ğŸ”„ ì•± ë™ê¸°í™”] ë²„íŠ¼ í‘œì‹œ
    - [ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”]: ëª©ë¡ + ë¶„ì„ ê²°ê³¼ ëª¨ë‘ ì´ˆê¸°í™”
    """
    # ìµœì´ˆ ì§„ì…: ì•± ì œê³µ datasetsë¡œ ìë™ ì´ˆê¸°í™”
    if st.session_state.get(_SS_DATASETS) is None:
        st.session_state[_SS_DATASETS] = list(datasets_from_app)
        st.session_state[_SS_APP_HASH] = _datasets_app_hash(datasets_from_app)

    ml_datasets: list = st.session_state[_SS_DATASETS]

    # ì•± datasets ë³€ê²½ ê°ì§€
    current_app_hash = _datasets_app_hash(datasets_from_app)
    saved_app_hash   = st.session_state.get(_SS_APP_HASH, "")
    app_changed      = (current_app_hash != saved_app_hash) and bool(datasets_from_app)

    # â”€â”€ í—¤ë”: ì œëª© + ë™ê¸°í™” + ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    hdr_title, hdr_sync, hdr_clear = st.columns([4, 2, 2])
    hdr_title.markdown(
        f"#### ğŸ“‹ ë¶„ì„ ë°ì´í„°ì…‹ "
        f"<span style='font-size:14px;font-weight:normal;color:#888;'>"
        f"({len(ml_datasets)}ê°œ ë¡œë“œ Â· ìµœì†Œ 3ê°œ í•„ìš”)</span>",
        unsafe_allow_html=True,
    )

    # ì•± datasets ë³€ê²½ ì‹œì—ë§Œ ë™ê¸°í™” ë²„íŠ¼ í‘œì‹œ
    if app_changed:
        n_app = len(datasets_from_app)
        if hdr_sync.button(
            f"ğŸ”„ ì•± ë™ê¸°í™” ({n_app}ê°œ)",
            key="ml_sync_btn",
            use_container_width=True,
            help=(
                "ì•±ì—ì„œ ì œê³µí•˜ëŠ” ë°ì´í„°ì…‹ ëª©ë¡ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                "í´ë¦­ ì‹œ ì•± ì œê³µ ëª©ë¡ìœ¼ë¡œ êµì²´ë©ë‹ˆë‹¤.\n"
                "(ê¸°ì¡´ ML ë¶„ì„ ê²°ê³¼ëŠ” ì´ˆê¸°í™”ë©ë‹ˆë‹¤)"
            ),
        ):
            st.session_state[_SS_DATASETS] = list(datasets_from_app)
            st.session_state[_SS_APP_HASH] = current_app_hash
            _invalidate_ml_results()
            st.rerun()
    else:
        hdr_sync.empty()

    if hdr_clear.button(
        "ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”",
        key="ml_clear_all",
        use_container_width=True,
        help="ë°ì´í„°ì…‹ ëª©ë¡ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ ëª¨ë‘ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.",
    ):
        st.session_state[_SS_DATASETS] = []
        st.session_state[_SS_APP_HASH] = ""
        _invalidate_ml_results()
        st.rerun()

    st.markdown(
        "<div style='border-top:1px solid #e0e0e0;margin:4px 0 8px 0;'></div>",
        unsafe_allow_html=True,
    )

    # â”€â”€ ë°ì´í„°ì…‹ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not ml_datasets:
        st.info(
            "â„¹ï¸ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.  \n"
            "ì•„ë˜ **â• ë°ì´í„°ì…‹ ì¶”ê°€** ë¥¼ í¼ì³ íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”."
        )
    else:
        # ì´ì „ ML ê²°ê³¼ (ìˆìœ¼ë©´ ì´ë¦„Â·ì ìˆ˜ì— ë°˜ì˜)
        names_result = st.session_state.get(_SS_NAMES) or []
        if_result    = st.session_state.get(_SS_IF_RESULT)

        def _row_label(name: str) -> str:
            """ì´ìƒ íƒì§€ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìƒíƒœ ì•„ì´ì½˜ + ì ìˆ˜ë¥¼ ì ‘ë‘ì–´ë¡œ ì¶”ê°€."""
            if if_result and names_result and name in names_result:
                idx   = names_result.index(name)
                score = float(if_result["scores"][idx])
                if idx in if_result["anomaly_indices"]:
                    return (
                        f"âš ï¸ **{name}** "
                        f"<span style='color:#cc3300;font-size:11px;'>"
                        f"ì´ìƒ ì ìˆ˜: {score:.3f}</span>"
                    )
                return (
                    f"âœ… **{name}** "
                    f"<span style='color:#2a7a2a;font-size:11px;'>"
                    f"ì ìˆ˜: {score:.3f}</span>"
                )
            return f"ğŸ”˜ {name}"

        # ì»¬ëŸ¼ í—¤ë”
        h_no, h_name, h_pts, h_del = st.columns([0.45, 4.5, 1.3, 0.75])
        for col, txt in zip(
            (h_no, h_name, h_pts, h_del),
            ("#", "ì´ë¦„", "í¬ì¸íŠ¸", "ì‚­ì œ"),
        ):
            col.markdown(
                f"<span style='font-size:12px;color:#888;font-weight:600;'>{txt}</span>",
                unsafe_allow_html=True,
            )

        to_remove = None
        for i, ds in enumerate(ml_datasets):
            name = ds.get("name", f"dataset_{i+1}")
            c_no, c_name, c_pts, c_del = st.columns([0.45, 4.5, 1.3, 0.75])

            c_no.markdown(
                f"<div style='padding-top:6px;font-size:13px;color:#666;'>{i+1}</div>",
                unsafe_allow_html=True,
            )
            c_name.markdown(
                f"<div style='padding-top:4px;font-size:13px;'>{_row_label(name)}</div>",
                unsafe_allow_html=True,
            )

            try:
                n_pts = len(pd.read_json(ds["df_json"]))
                c_pts.markdown(
                    f"<div style='padding-top:6px;font-size:13px;color:#555;'>{n_pts:,}</div>",
                    unsafe_allow_html=True,
                )
            except Exception:
                c_pts.markdown(
                    "<div style='padding-top:6px;font-size:12px;color:#aaa;'>â€”</div>",
                    unsafe_allow_html=True,
                )

            # âœ• ë²„íŠ¼: keyì— ì¸ë±ìŠ¤+ì´ë¦„ í¬í•¨ â†’ ëª©ë¡ ë³€ê²½ í›„ rerun ì‹œ key ì¶©ëŒ ë°©ì§€
            if c_del.button("âœ•", key=f"ml_del_{i}_{name}", use_container_width=True):
                to_remove = i

        if to_remove is not None:
            st.session_state[_SS_DATASETS].pop(to_remove)
            # ì›¨ì´í¼ ëª©ë¡ ë³€ê²½ â†’ PCA ë¬´íš¨í™”
            _invalidate_ml_results()
            st.session_state[_SS_PCA_KEY] = None
            st.session_state[_SS_IF_KEY]  = None
            st.rerun()

    st.markdown(
        "<div style='border-top:1px solid #e0e0e0;margin:8px 0 4px 0;'></div>",
        unsafe_allow_html=True,
    )

    # 3ê°œ ë¯¸ë§Œì´ë©´ ìë™ í¼ì¹¨
    with st.expander("â• ë°ì´í„°ì…‹ ì¶”ê°€", expanded=(len(ml_datasets) < 3)):
        _render_dataset_adder(data_folder)


# [í•¨ìˆ˜ 7] _compute_pca_key (ë‚´ë¶€ í—¬í¼)
# =============================================================================

def _compute_pca_key(names: list, resolution: int) -> str:
    """
    PCA ìºì‹œ í‚¤ ìƒì„± (ì›¨ì´í¼ ì´ë¦„ ëª©ë¡ + í•´ìƒë„).

    [ìºì‹œ í‚¤ ì„¤ê³„ ì›ì¹™]
    - ì›¨ì´í¼ ëª©ë¡ ë³€ê²½ â†’ PCA ë¬´íš¨í™”
    - í•´ìƒë„ ë³€ê²½ â†’ PCA ë¬´íš¨í™” (íŠ¹ì„± ì°¨ì›ì´ ë‹¬ë¼ì§)
    - contamination ë³€ê²½ â†’ PCA ê·¸ëŒ€ë¡œ (IFë§Œ ì¬ì‹¤í–‰)
    - ì›¨ì´í¼ ìˆœì„œ ë³€ê²½: sorted()ë¡œ ìˆœì„œ ë¬´ê´€í•˜ê²Œ ì¼ê´€ëœ í‚¤ ìƒì„±
      (ë™ì¼ ì›¨ì´í¼ ì„¸íŠ¸ë¼ë©´ ìˆœì„œì™€ ë¬´ê´€í•˜ê²Œ ê°™ì€ PCA ì¬ì‚¬ìš©)

    ì¸ì:
        names     : ìœ íš¨ ì›¨ì´í¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        resolution: íŠ¹ì„± ì¶”ì¶œ í•´ìƒë„

    ë°˜í™˜:
        ìºì‹œ í‚¤ ë¬¸ìì—´ (MD5 í•´ì‹œ â†’ ì§§ê³  ê³ ì • ê¸¸ì´)
    """
    key_str = f"{sorted(names)},{resolution}"
    return hashlib.md5(key_str.encode()).hexdigest()[:16]


# =============================================================================
# [í•¨ìˆ˜ 8] render_anomaly_tab (UI ë Œë”ëŸ¬)
# =============================================================================

def render_anomaly_tab(
    datasets: list,
    resolution: int,
    data_folder: str,
) -> None:
    """
    ML ì´ìƒ íƒì§€ íƒ­ì˜ ì „ì²´ UIë¥¼ ë Œë”ë§.

    [ë ˆì´ì•„ì›ƒ êµ¬ì¡°]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  scikit-learn ë¯¸ì„¤ì¹˜ ì˜¤ë¥˜ (í•„ìš” ì‹œë§Œ)                                 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ contamination â”‚ resolution   â”‚  [ğŸ¤– ì´ìƒ íƒì§€ ì‹¤í–‰] ë²„íŠ¼             â”‚
    â”‚ ìŠ¬ë¼ì´ë”      â”‚ ìŠ¬ë¼ì´ë”     â”‚                                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PCA ì‚°ì ë„               â”‚  ì´ìƒ ì ìˆ˜ ë§‰ëŒ€ ê·¸ë˜í”„                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ì´ìƒ íƒì§€ ê²°ê³¼ ìš”ì•½ (í†µê³„ ì§€í‘œ)                                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  ì´ìƒ ì›¨ì´í¼ Heatmap ë¯¸ë¦¬ë³´ê¸° (compact=True, ìµœëŒ€ 6ê°œ)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    [contamination ë³€ê²½ ì‹œ ìºì‹œ ì¬ì‚¬ìš© ë¡œì§]
    pca_key = _compute_pca_key(names, ml_resolution)
    if session_state["ml_pca_key"] != pca_key:
        â†’ PCA ì¬ì‹¤í–‰ (ì›¨ì´í¼/í•´ìƒë„ ë³€ê²½)
    if session_state["ml_if_key"] != (pca_key, contamination):
        â†’ IF ì¬ì‹¤í–‰ (contamination ë³€ê²½ ë˜ëŠ” PCA ë³€ê²½)
    â†’ contaminationë§Œ ë°”ë€Œë©´: PCA ê±´ë„ˆëœ€, IFë§Œ ì¬ì‹¤í–‰

    [session_state í‚¤]
    "ml_pca_key"    : í˜„ì¬ PCA ìºì‹œ í‚¤
    "ml_pca_result" : PCA ê²°ê³¼ dict (components, explained_variance_ratio, ...)
    "ml_if_key"     : (pca_key, contamination) íŠœí”Œ
    "ml_if_result"  : IF ê²°ê³¼ dict (predictions, scores, anomaly_indices, ...)
    "ml_names"      : ìœ íš¨ ì›¨ì´í¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    "ml_patterns"   : {wafer_name: pattern_str} íŒ¨í„´ ë¶„ë¥˜ dict

    ì¸ì:
        datasets   : st.session_state["datasets"] (ì›¨ì´í¼ ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸)
        resolution : ê¸°ë³¸ ë³´ê°„ í•´ìƒë„ (ì‚¬ì´ë“œë°” ìŠ¬ë¼ì´ë”)
        data_folder: ë°ì´í„° í´ë” ê²½ë¡œ
    """
    # â”€â”€ scikit-learn ë¯¸ì„¤ì¹˜ ì²´í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not _SKLEARN_OK:
        st.error(
            "âŒ scikit-learnì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n"
            "í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ì»¤ë§¨ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\n"
            "```\npip install scikit-learn\n```"
        )
        return

    # â”€â”€ ì œëª© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("#### ğŸ¤– ML ì´ìƒ íƒì§€ (PCA + IsolationForest)")
    st.caption(
        "ì—¬ëŸ¬ ì›¨ì´í¼ ë§µì„ ê·¸ë¦¬ë“œ ë²¡í„°ë¡œ ë³€í™˜ â†’ PCA ì°¨ì› ì¶•ì†Œ â†’ "
        "IsolationForestë¡œ ì´ìƒ ì›¨ì´í¼ ìë™ íƒì§€"
    )
    st.markdown("---")

    # â”€â”€ [ì‹ ê·œ] ë°ì´í„°ì…‹ ê´€ë¦¬ íŒ¨ë„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # _render_dataset_panel ì´ _SS_DATASETS ë¥¼ ì´ˆê¸°í™”Â·ê´€ë¦¬.
    # ì´í›„ ëª¨ë“  ML ë¡œì§ì€ st.session_state[_SS_DATASETS] ë¥¼ ì‚¬ìš©.
    _render_dataset_panel(datasets_from_app=datasets, data_folder=data_folder)

    ml_datasets: list = st.session_state.get(_SS_DATASETS) or []

    # â”€â”€ ìµœì†Œ ì›¨ì´í¼ ìˆ˜ ì²´í¬ (ë°ì´í„°ì…‹ íŒ¨ë„ ì•„ë˜ì—ì„œ ì§„í–‰ ì°¨ë‹¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    n_datasets = len(ml_datasets)
    if n_datasets < 3:
        st.info(
            f"â„¹ï¸ **ìµœì†Œ 3ê°œì˜ ì›¨ì´í¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.** í˜„ì¬ {n_datasets}ê°œ.  \n"
            "ìœ„ì˜ **â• ë°ì´í„°ì…‹ ì¶”ê°€** ë¡œ íŒŒì¼ì„ ì¶”ê°€í•˜ê±°ë‚˜ "
            "**ğŸ”„ ì•± ë™ê¸°í™”** ë²„íŠ¼ì„ ëˆŒëŸ¬ ì•± ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”."
        )
        return

    # â”€â”€ ì†Œê·œëª¨ ìƒ˜í”Œ ì‹ ë¢°ë„ ê²½ê³  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if n_datasets < 5:
        st.warning(
            f"âš ï¸ ì›¨ì´í¼ ìˆ˜({n_datasets}ê°œ)ê°€ ì ì–´ ì´ìƒ íƒì§€ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
            "10ê°œ ì´ìƒ ê¶Œì¥. íƒì§€ ê²°ê³¼ë¥¼ ì°¸ê³  ìë£Œë¡œë§Œ í™œìš©í•˜ì„¸ìš”."
        )

    st.markdown("---")

    # â”€â”€ ì»¨íŠ¸ë¡¤ íŒ¨ë„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ctrl_c1, ctrl_c2, ctrl_c3 = st.columns([1, 1, 2])

    with ctrl_c1:
        contamination: float = st.slider(
            "ì´ìƒ ë¹„ìœ¨ (contamination)",
            min_value=0.05,
            max_value=0.30,
            value=st.session_state.get(_SS_CONTAM, 0.10),
            step=0.05,
            key=_SS_CONTAM,
            help=(
                "ì „ì²´ ì›¨ì´í¼ ì¤‘ ì´ìƒìœ¼ë¡œ ê°„ì£¼í•  ë¹„ìœ¨.\n\n"
                "ê³µì • ê²½í—˜ì  ê²°í•¨ìœ¨ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.\n"
                "ê¸°ë³¸ê°’ 0.10 = 10%"
            ),
        )

    with ctrl_c2:
        ml_resolution: int = st.slider(
            "íŠ¹ì„± ì¶”ì¶œ í•´ìƒë„",
            min_value=20,
            max_value=80,
            value=st.session_state.get(_SS_RESOLUTION, 40),
            step=10,
            key=_SS_RESOLUTION,
            help=(
                "ì›¨ì´í¼ ë§µ ë³´ê°„ ê·¸ë¦¬ë“œ í•´ìƒë„.\n\n"
                "ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„ (20Ã—20=400ì°¨ì›)\n"
                "ë†’ì„ìˆ˜ë¡ ì •ë°€ (80Ã—80=6400ì°¨ì›)\n"
                "ê¸°ë³¸ê°’ 40 (40Ã—40=1600ì°¨ì›)"
            ),
        )

    with ctrl_c3:
        st.markdown("")   # ë²„íŠ¼ ìˆ˜ì§ ì •ë ¬ìš© ì—¬ë°±
        run_clicked = st.button(
            "ğŸ¤– ì´ìƒ íƒì§€ ì‹¤í–‰",
            type="primary",
            use_container_width=True,
            key="ml_run_btn",
            help=(
                "ë²„íŠ¼ í´ë¦­ ì‹œ ë™ì‘:\n"
                "1. ê° ì›¨ì´í¼ ZI ê·¸ë¦¬ë“œ ì¶”ì¶œ (íŠ¹ì„± ë²¡í„°í™”)\n"
                "2. PCA ì°¨ì› ì¶•ì†Œ\n"
                "3. IsolationForest ì´ìƒ íƒì§€"
            ),
        )

    # â”€â”€ contamination ë³€ê²½ ê°ì§€: IFë§Œ ì¬ì‹¤í–‰ íŠ¸ë¦¬ê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë²„íŠ¼ í´ë¦­ ì—†ì´ ìŠ¬ë¼ì´ë” ë³€ê²½ ì‹œ: contaminationë§Œ ë°”ë€Œë©´ IF ìë™ ì¬ì‹¤í–‰
    if_key_current = (
        st.session_state.get(_SS_PCA_KEY, ""),
        contamination,
    )
    if_needs_rerun = (
        st.session_state.get(_SS_IF_RESULT) is not None
        and st.session_state.get(_SS_IF_KEY) != if_key_current
    )

    if if_needs_rerun:
        # contamination ë³€ê²½ â†’ IFë§Œ ì¬ì‹¤í–‰ (PCA ì¬ì‚¬ìš©)
        pca_result = st.session_state.get(_SS_PCA_RESULT)
        if pca_result is not None:
            with st.spinner("contamination ë³€ê²½ â†’ IsolationForest ì¬ì‹¤í–‰ ì¤‘..."):
                try:
                    if_result = run_isolation_forest(
                        pca_result["components"], contamination
                    )
                    st.session_state[_SS_IF_RESULT] = if_result
                    st.session_state[_SS_IF_KEY]    = if_key_current
                    # [ìˆ˜ì •] _SS_CONTAM ìˆ˜ë™ write ì œê±°: key=_SS_CONTAM ìœ„ì ¯ì´ ìë™ ê´€ë¦¬
                except Exception as e:
                    st.error(f"IsolationForest ì¬ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    # â”€â”€ ë©”ì¸ ì‹¤í–‰ ë²„íŠ¼ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if run_clicked:
        # â”€â”€ _SS_DATASETS ì—ì„œ df_json ëª©ë¡ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # [ìˆ˜ì •] datasets(ì•± íŒŒë¼ë¯¸í„°) ëŒ€ì‹  st.session_state[_SS_DATASETS] ì‚¬ìš©
        # â†’ ML íƒ­ì—ì„œ ì§ì ‘ ì¶”ê°€/ì‚­ì œí•œ ëª©ë¡ì´ ë°˜ì˜ë¨
        maps_data = []
        for ds in ml_datasets:
            name    = ds.get("name", "Unknown")
            df_json = ds.get("df_json", None)
            if df_json is None:
                continue
            maps_data.append({"df_json": df_json, "name": name})

        if len(maps_data) < 3:
            st.error(
                "âŒ ìœ íš¨í•œ ì›¨ì´í¼ ë°ì´í„°ê°€ 3ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. "
                "ìœ„ì˜ ë°ì´í„°ì…‹ íŒ¨ë„ì—ì„œ íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”."
            )
            return

        # â”€â”€ íŠ¹ì„± ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner(f"ì›¨ì´í¼ íŠ¹ì„± ì¶”ì¶œ ì¤‘... ({len(maps_data)}ê°œ Ã— {ml_resolution}Â² ê·¸ë¦¬ë“œ)"):
            try:
                feature_matrix, valid_names, valid_mask = prepare_wafer_features(
                    maps_data, resolution=ml_resolution
                )
            except Exception as e:
                st.error(f"íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                return

        if len(valid_names) < 3:
            st.error(
                f"âŒ ìœ íš¨í•œ ì›¨ì´í¼ê°€ {len(valid_names)}ê°œë¿ì…ë‹ˆë‹¤ (ìµœì†Œ 3ê°œ í•„ìš”). "
                "ë³´ê°„ì— ì‹¤íŒ¨í•œ ì›¨ì´í¼ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            return

        # â”€â”€ PCA ì‹¤í–‰ (ìºì‹œ í‚¤ í™•ì¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        pca_key_new = _compute_pca_key(valid_names, ml_resolution)
        pca_key_old = st.session_state.get(_SS_PCA_KEY, "")

        if pca_key_new != pca_key_old:
            # ì›¨ì´í¼ ëª©ë¡ ë˜ëŠ” í•´ìƒë„ ë³€ê²½ â†’ PCA ì¬ì‹¤í–‰
            with st.spinner("PCA ì°¨ì› ì¶•ì†Œ ì‹¤í–‰ ì¤‘..."):
                try:
                    pca_result = run_pca(feature_matrix)
                    st.session_state[_SS_PCA_RESULT] = pca_result
                    st.session_state[_SS_PCA_KEY]    = pca_key_new
                    # PCA ë³€ê²½ ì‹œ IF ê²°ê³¼ë„ ë¬´íš¨í™”
                    st.session_state[_SS_IF_RESULT]  = None
                    st.session_state[_SS_IF_KEY]     = None
                except Exception as e:
                    st.error(f"PCA ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                    return
        else:
            # ì›¨ì´í¼/í•´ìƒë„ ë™ì¼ â†’ PCA ì¬ì‚¬ìš©
            pca_result = st.session_state[_SS_PCA_RESULT]
            st.info("âœ… PCA ê²°ê³¼ ì¬ì‚¬ìš© (ì›¨ì´í¼ ëª©ë¡Â·í•´ìƒë„ ë™ì¼)")

        # â”€â”€ IsolationForest ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if_key_new  = (pca_key_new, contamination)
        if_key_curr = st.session_state.get(_SS_IF_KEY)

        if if_key_new != if_key_curr:
            with st.spinner("IsolationForest ì´ìƒ íƒì§€ ì‹¤í–‰ ì¤‘..."):
                try:
                    if_result = run_isolation_forest(
                        pca_result["components"], contamination
                    )
                    st.session_state[_SS_IF_RESULT] = if_result
                    st.session_state[_SS_IF_KEY]    = if_key_new
                except Exception as e:
                    st.error(f"IsolationForest ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                    return
        else:
            if_result = st.session_state[_SS_IF_RESULT]

        # â”€â”€ íŒ¨í„´ ë¶„ë¥˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        patterns = {}
        for i, name in enumerate(valid_names):
            df_json = maps_data[i]["df_json"] if i < len(maps_data) else ""
            patterns[name] = classify_anomaly_pattern(df_json)
        st.session_state[_SS_PATTERNS]    = patterns
        st.session_state[_SS_NAMES]       = valid_names
        # [ìˆ˜ì •] _SS_RESOLUTION ìˆ˜ë™ write ì œê±°: key=_SS_RESOLUTION ìœ„ì ¯ì´ ìë™ ê´€ë¦¬

        # ìœ íš¨í•˜ì§€ ì•Šì€ ì›¨ì´í¼ ìˆ˜ ê²½ê³ 
        n_invalid = sum(1 for v in valid_mask if not v)
        if n_invalid > 0:
            st.warning(f"âš ï¸ ë³´ê°„ ì‹¤íŒ¨ë¡œ {n_invalid}ê°œ ì›¨ì´í¼ ì œì™¸ë¨.")

    # â”€â”€ ê²°ê³¼ ë Œë”ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pca_result = st.session_state.get(_SS_PCA_RESULT)
    if_result  = st.session_state.get(_SS_IF_RESULT)
    names      = st.session_state.get(_SS_NAMES, [])
    patterns   = st.session_state.get(_SS_PATTERNS, {})

    if pca_result is None or if_result is None:
        st.info(
            "â„¹ï¸ íŒŒë¼ë¯¸í„° ì„¤ì • í›„ **'ğŸ¤– ì´ìƒ íƒì§€ ì‹¤í–‰'** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.\n\n"
            "PCAì™€ IsolationForestë¡œ ì´ìƒ ì›¨ì´í¼ë¥¼ ìë™ íƒì§€í•©ë‹ˆë‹¤."
        )
        return

    # â”€â”€ ìš”ì•½ ì§€í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    n_valid   = len(names)
    n_anomaly = len(if_result["anomaly_indices"])
    n_normal  = n_valid - n_anomaly
    anom_pct  = n_anomaly / n_valid * 100 if n_valid > 0 else 0
    mean_score = float(if_result["scores"].mean()) if n_valid > 0 else 0.0

    m1, m2, m3, m4, m5 = st.columns(5)
    m1.metric("ë¶„ì„ ì›¨ì´í¼", f"{n_valid}ê°œ")
    m2.metric(
        "ì´ìƒ íƒì§€",
        f"{n_anomaly}ê°œ",
        delta=f"{anom_pct:.1f}%",
        delta_color="inverse" if n_anomaly > 0 else "off",
    )
    m3.metric("ì •ìƒ", f"{n_normal}ê°œ")
    m4.metric(
        "í‰ê·  ì´ìƒ ì ìˆ˜",
        f"{mean_score:.4f}",
        help="0 = ì •ìƒ, 1 = ì™„ì „ ì´ìƒ",
    )
    m5.metric(
        "PCA ì„¤ëª…ë ¥",
        f"{sum(pca_result['explained_variance_ratio'][:2])*100:.1f}%",
        help="PC1+PC2ê°€ ì „ì²´ ë¶„ì‚°ì˜ ëª‡ %ë¥¼ ì„¤ëª…í•˜ëŠ”ì§€",
    )

    st.markdown("---")

    # â”€â”€ ìƒë‹¨: PCA ì‚°ì ë„ | ì´ìƒ ì ìˆ˜ ë§‰ëŒ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    col_pca, col_bar = st.columns([1, 1])

    # df_jsons ëª©ë¡ ìˆ˜ì§‘ (íŒ¨í„´ ë¶„ë¥˜ìš© hover ë°ì´í„°)
    # [ìˆ˜ì •] datasets â†’ ml_datasets (ML íƒ­ ì „ìš© ëª©ë¡)
    df_jsons_for_hover = []
    for ds in ml_datasets:
        if ds.get("name") in names:
            df_jsons_for_hover.append(ds.get("df_json", ""))
    # ì´ë¦„ ìˆœì„œ ë§ì¶”ê¸°
    name_to_df_json = {
        ds.get("name"): ds.get("df_json", "")
        for ds in ml_datasets if ds.get("df_json")
    }
    df_jsons_ordered = [name_to_df_json.get(n, "") for n in names]

    with col_pca:
        st.markdown("##### ğŸ”µ PCA ì´ìƒ íƒì§€ ì‚°ì ë„")
        fig_pca = create_pca_scatter(
            pca_result, if_result, names, df_jsons_ordered
        )
        st.plotly_chart(fig_pca, use_container_width=True)

    with col_bar:
        st.markdown("##### ğŸ“Š ì›¨ì´í¼ë³„ ì´ìƒ ì ìˆ˜")
        fig_bar = create_anomaly_score_bar(
            wafer_names=names,
            scores=if_result["scores"],
            is_anomaly=(if_result["predictions"] == -1),
        )
        st.plotly_chart(fig_bar, use_container_width=True)

    # â”€â”€ ì´ìƒ íŒ¨í„´ ë¶„ë¥˜ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if patterns:
        st.markdown("---")
        st.markdown("##### ğŸ”¬ ì´ìƒ íŒ¨í„´ ë¶„ë¥˜")

        # ì´ìƒ ì›¨ì´í¼ ê°•ì¡° + ì •ìƒ ì›¨ì´í¼ë„ í¬í•¨í•œ ì „ì²´ í…Œì´ë¸”
        pattern_rows = []
        is_anom_set = set(if_result["anomaly_indices"])
        for i, name in enumerate(names):
            is_anom = i in is_anom_set
            score   = float(if_result["scores"][i])
            pattern = patterns.get(name, "N/A")
            pattern_rows.append({
                "ìƒíƒœ":        "âš ï¸ ì´ìƒ" if is_anom else "âœ… ì •ìƒ",
                "ì›¨ì´í¼":      name,
                "ì´ìƒ ì ìˆ˜":   round(score, 4),
                "íŒ¨í„´ ë¶„ë¥˜":   pattern,
            })

        # ì´ìƒ ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        pattern_rows.sort(key=lambda r: r["ì´ìƒ ì ìˆ˜"], reverse=True)

        st.dataframe(
            pd.DataFrame(pattern_rows),
            use_container_width=True,
            hide_index=True,
            column_config={
                "ì´ìƒ ì ìˆ˜": st.column_config.ProgressColumn(
                    "ì´ìƒ ì ìˆ˜",
                    format="%.4f",
                    min_value=0.0,
                    max_value=1.0,
                ),
            },
        )

    # â”€â”€ ì´ìƒ ì›¨ì´í¼ Heatmap ë¯¸ë¦¬ë³´ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    anomaly_indices = if_result["anomaly_indices"]
    if anomaly_indices:
        st.markdown("---")
        st.markdown("##### ğŸ—ºï¸ ì´ìƒ ì›¨ì´í¼ Heatmap ë¯¸ë¦¬ë³´ê¸°")

        # ìµœëŒ€ 6ê°œë§Œ ë¯¸ë¦¬ë³´ê¸° (í™”ë©´ ê³µê°„ ì œí•œ)
        preview_count = min(len(anomaly_indices), 6)
        preview_cols  = st.columns(min(preview_count, 3))

        for col_idx, anom_idx in enumerate(anomaly_indices[:preview_count]):
            with preview_cols[col_idx % 3]:
                name    = names[anom_idx]
                score   = float(if_result["scores"][anom_idx])
                pattern = patterns.get(name, "N/A")

                st.markdown(
                    f"**{name}**  \n"
                    f"ì ìˆ˜: `{score:.4f}` | íŒ¨í„´: `{pattern}`"
                )

                # df_jsonìœ¼ë¡œ compact Heatmap ìƒì„±
                ds_match = next(
                    (ds for ds in ml_datasets if ds.get("name") == name), None
                )
                if ds_match and ds_match.get("df_json"):
                    try:
                        fig_preview = create_2d_heatmap(
                            df_json=ds_match["df_json"],
                            resolution=resolution,
                            colorscale="RdBu_r",    # ì´ìƒ ê°•ì¡°: ë¹¨ê°•-íŒŒë‘
                            show_points=False,
                            compact=True,           # ë¹„êµ ëª¨ë“œìš© ì†Œí˜•
                        )
                        st.plotly_chart(
                            fig_preview,
                            use_container_width=True,
                            key=f"ml_preview_{name}_{col_idx}",
                        )
                    except Exception:
                        st.warning(f"'{name}' ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨")
                else:
                    st.info("ë°ì´í„° ì—†ìŒ")

    # â”€â”€ ê²°ê³¼ í•´ì„ ê°€ì´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“– ê²°ê³¼ í•´ì„ ê°€ì´ë“œ", expanded=False):
        st.markdown(
            """
**ì´ìƒ ì ìˆ˜ í•´ì„:**
- `0.0 ~ 0.4`: ì •ìƒ ë²”ìœ„ (ê³µì • ë³€ë™ ë‚´)
- `0.4 ~ 0.7`: ì£¼ì˜ ìš”ë§ (ê²½ê³„ ìˆ˜ì¤€)
- `0.7 ~ 1.0`: ì´ìƒ ì˜ì‹¬ (ê³µì • ì ê²€ ê¶Œì¥)

**íŒ¨í„´ ë¶„ë¥˜ ì˜ë¯¸:**
| íŒ¨í„´ | ì˜ë¯¸ | ê³µì • ì›ì¸ |
|------|------|-----------|
| Ring | ë°©ì‚¬í˜• ëŒ€ì¹­ ì´ìƒ | ê°€ìŠ¤ ë¶„í¬ ì¤‘ì‹¬ì§‘ì¤‘/í™•ì‚° ë¶ˆê· ì¼ |
| Edge Degradation | ê°€ì¥ìë¦¬ ë‘ê»˜ ì €í•˜ | Edge exclusion, ë¡œë”© íš¨ê³¼ |
| X/Y-Gradient | ë°©í–¥ì„± ë‘ê»˜ êµ¬ë°° | ê¸°íŒ ê¸°ìš¸ê¸°, ê°€ìŠ¤ ë°©í–¥ì„± |
| Hotspot | êµ­ì†Œ í”¼í¬ | íŒŒí‹°í´, ìŠ¤í¬ë˜ì¹˜, ì¸¡ì • ì˜¤ë¥˜ |
| Global Shift | ì „ì²´ ìˆ˜ì¤€ ì´íƒˆ | ê³µì • ë ˆì‹œí”¼ ë³€ê²½, ë“œë¦¬í”„íŠ¸ |
| Normal | ì •ìƒ íŒ¨í„´ | â€” |

**contamination ì¡°ì • ê°€ì´ë“œ:**
- ê³µì • ê²°í•¨ìœ¨ì´ ì•Œë ¤ì ¸ ìˆë‹¤ë©´ ê·¸ ê°’ìœ¼ë¡œ ì„¤ì • (ì˜ˆ: ê²°í•¨ìœ¨ 5% â†’ 0.05)
- ëª¨ë¦„: 0.10ìœ¼ë¡œ ì‹œì‘ í›„ PCA ì‚°ì ë„ì—ì„œ ì´ìƒì¹˜ ë¶„ë¦¬ë„ í™•ì¸í•˜ë©° ì¡°ì •
            """
        )